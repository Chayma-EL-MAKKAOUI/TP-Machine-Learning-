{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40d46c2-8114-4c77-ac01-3c8fd4aa4aae",
   "metadata": {},
   "source": [
    "## Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f7a0afe1-c723-4bd8-943b-3720ae6679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'états: 16\n",
      "Nombre d'actions: 4\n",
      "Épisode 0, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 0, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 0, État: 1, Action: 1, Récompense: 0.0\n",
      "Épisode 0, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 0, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 0, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 0, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 1, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 2, État: 1, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 1, Récompense: 0.0\n",
      "Épisode 3, État: 8, Action: 3, Récompense: 0.0\n",
      "Épisode 3, État: 8, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 9, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 3, Récompense: 0.0\n",
      "Épisode 4, État: 2, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 2, Action: 3, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 1, Récompense: 0.0\n",
      "Épisode 5, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 5, État: 4, Action: 1, Récompense: 0.0\n",
      "Épisode 6, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 6, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 6, État: 8, Action: 1, Récompense: 0.0\n",
      "Épisode 7, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 7, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 8, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 8, État: 1, Action: 3, Récompense: 0.0\n",
      "Épisode 8, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 8, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 8, État: 1, Action: 3, Récompense: 0.0\n",
      "Épisode 8, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 8, État: 4, Action: 2, Récompense: 0.0\n",
      "Épisode 8, État: 8, Action: 1, Récompense: 0.0\n",
      "Épisode 8, État: 8, Action: 1, Récompense: 0.0\n",
      "Épisode 8, État: 9, Action: 3, Récompense: 0.0\n",
      "Épisode 9, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 9, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 9, État: 2, Action: 3, Récompense: 0.0\n",
      "Épisode 9, État: 2, Action: 0, Récompense: 0.0\n",
      "Épisode 9, État: 6, Action: 0, Récompense: 0.0\n",
      "Épisode 9, État: 10, Action: 2, Récompense: 0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=None)\n",
    "\n",
    "print(\"Nombre d'états:\", env.observation_space.n)\n",
    "print(\"Nombre d'actions:\", env.action_space.n)\n",
    "    \n",
    "for episode in range(10):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        print(f\"Épisode {episode}, État: {state}, Action: {action}, Récompense: {reward}\")\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937ced4-d28b-4821-9ab2-e4b44f135169",
   "metadata": {},
   "source": [
    "## Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17fa3a8a-b4c7-4c9a-badc-be6bfbb4bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table initialisée :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de la Q-Table\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Affichage de la Q-Table initiale\n",
    "print(\"Q-Table initialisée :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14b9d7-bc11-4b01-b504-3b5bbf308e74",
   "metadata": {},
   "source": [
    "## Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8dcf0ca7-55d4-41e0-9075-12a7795f1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table après entraînement :\n",
      "[[0.55729164 0.47376851 0.4734743  0.47413035]\n",
      " [0.37069667 0.36107123 0.38652561 0.52033741]\n",
      " [0.42912087 0.41821641 0.41685385 0.4865086 ]\n",
      " [0.37878667 0.27255409 0.28749879 0.46985931]\n",
      " [0.56787016 0.46084579 0.37943137 0.32512516]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39383036 0.13059572 0.15936234 0.13626658]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37141834 0.44536859 0.49613478 0.61164197]\n",
      " [0.54014071 0.63066861 0.41832246 0.4953716 ]\n",
      " [0.65196212 0.36622726 0.37004223 0.37752476]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.42505831 0.59409454 0.71776553 0.56939427]\n",
      " [0.738243   0.81193027 0.73191411 0.73046906]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Définition des hyperparamètres\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.99  # Facteur de discount\n",
    "epsilon = 0.8  # Exploration initiale\n",
    "epsilon_decay = 0.995  # Décroissance d'epsilon\n",
    "num_episodes = 5000  # Nombre d'épisodes\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Exploitation\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Mise à jour de la Q-Table\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "        state = next_state\n",
    "    \n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "print(\"Q-Table après entraînement :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d5cd1-15a8-4011-99ce-6d28dcb3df46",
   "metadata": {},
   "source": [
    "## Exercice 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fa62429e-0128-4b64-96fc-bb3034d2aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de réussite de l'agent après apprentissage : 89.00%\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state, :]) \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "    \n",
    "    if reward > 0:\n",
    "        successes += 1\n",
    "\n",
    "success_rate = successes / num_test_episodes\n",
    "print(f\"Taux de réussite de l'agent après apprentissage : {success_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb0e06-59af-4df8-bef0-e7f8a75e653e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feaf47f-4259-4608-9ff9-654c3479f178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
